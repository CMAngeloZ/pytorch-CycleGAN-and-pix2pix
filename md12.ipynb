{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --dataroot DATAROOT [--name NAME]\n",
      "                             [--gpu_ids GPU_IDS]\n",
      "                             [--checkpoints_dir CHECKPOINTS_DIR]\n",
      "                             [--model MODEL] [--input_nc INPUT_NC]\n",
      "                             [--output_nc OUTPUT_NC] [--ngf NGF] [--ndf NDF]\n",
      "                             [--netD NETD] [--netG NETG]\n",
      "                             [--n_layers_D N_LAYERS_D] [--norm NORM]\n",
      "                             [--init_type INIT_TYPE] [--init_gain INIT_GAIN]\n",
      "                             [--no_dropout] [--dataset_mode DATASET_MODE]\n",
      "                             [--direction DIRECTION] [--serial_batches]\n",
      "                             [--num_threads NUM_THREADS]\n",
      "                             [--batch_size BATCH_SIZE] [--load_size LOAD_SIZE]\n",
      "                             [--crop_size CROP_SIZE]\n",
      "                             [--max_dataset_size MAX_DATASET_SIZE]\n",
      "                             [--preprocess PREPROCESS] [--no_flip]\n",
      "                             [--display_winsize DISPLAY_WINSIZE]\n",
      "                             [--epoch EPOCH] [--load_iter LOAD_ITER]\n",
      "                             [--verbose] [--suffix SUFFIX] [--use_wandb]\n",
      "                             [--wandb_project_name WANDB_PROJECT_NAME]\n",
      "                             [--results_dir RESULTS_DIR]\n",
      "                             [--aspect_ratio ASPECT_RATIO] [--phase PHASE]\n",
      "                             [--eval] [--num_test NUM_TEST]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --dataroot\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marco\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:3534: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "### Assuming you have cloned the Cycle GAN repo from github: https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix\n",
    "### run download_cyclegan_model.sh script to download pretrained summer2winter_yosemite_pretrained model (instructions on github page)\n",
    "### Create a jupyter notebook and follow the below code snippets\n",
    "from models import create_model\n",
    "from options.test_options import TestOptions\n",
    "import torch\n",
    "from polygraphy.backend.onnx import fold_constants\n",
    "import onnx\n",
    "\n",
    "opt = TestOptions().parse()\n",
    "opt.netG = 'resnet_9blocks'\n",
    "opt.norm = 'instance'\n",
    "opt.checkpoints_dir = './checkpoints' ### directory storing the pre-trained model\n",
    "opt.name = 'summer2winter_yosemite_pretrained'\n",
    "model = create_model(opt)\n",
    "model.setup(opt)\n",
    "### conversion to onnx\n",
    "dummy_inp = torch.rand(1, 3, 256, 256)\n",
    "onnx_filepath = './summer2winter_yosemite.onnx'\n",
    "torch.onnx.export(model, dummy_inp, onnx_filepath, opset_version=10,\n",
    "input_names = ['inp_img'])\n",
    "\n",
    "### optional step to do Constant folding which involves pre-computing expressions that do not depend on runtime information\n",
    "converted_onnx_model = onnx.load('./summer2winter_yosemite.onnx')\n",
    "constant_folded_model = fold_constants(onnx_model)\n",
    "onnx.save(constant_folded_model, './summer2winter_yosemite_folded.onnx')\n",
    "### This will give you converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python3 -m venv openvino_env\n",
    "source openvino_env/bin/activate\n",
    "python -m pip install --upgrade pip\n",
    "pip install openvino-dev==2022.1.0. ### tested on this version currently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mo -h ### This will return the help message for Model Optimizer if\n",
    "installation finished successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### To convert, run the following in your terminal (you can use either\n",
    "onnx model or the folded onnx model)\n",
    "### input_model - specify your input onnx file\n",
    "### data_type - use fp16 to convert model to fp16 precision for better\n",
    "performance\n",
    "### output_dir - directory where your openvino files will be saved\n",
    "mo --input_model summer2winter_yosemite_folded.onnx --data_type fp16 --output_dir \"./summer2winter_yosemite_openvino\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pytorch Model Inference\n",
    "from util.util import tensor2im\n",
    "from data.base_dataset import get_transform\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "input_image_filename = 'Yosemite_summer.jpg'\n",
    "input_image = Image.open(input_image_filename).convert('RGB')\n",
    "transformation = get_transform(opt, grayscale = False)\n",
    "transformed_img = transformation(input_image).unsqueeze(0)\n",
    "if opt.eval:\n",
    "    model.eval()\n",
    "with torch.no_grad():\n",
    "    output_image = model.netG_A(transformed_img)\n",
    "output_image = tensor2im(output_image) ### final Pytorch result image\n",
    "\n",
    "### OpenVino Model Inference\n",
    "from openvino.inference_engine import IECore, IENetwork\n",
    "from openvino.runtime import Core\n",
    "\n",
    "ie_obj = IECore()\n",
    "\n",
    "openvino_network = ie_obj.read_network(model =\n",
    "'summer2winter_yosemite_folded.xml', weights =\n",
    "'summer2winter_yosemite_folded.bin')\n",
    "executable_network = ie_obj.load_network(network = openvino_network,\n",
    "device='CPU')\n",
    "\n",
    "openvino_output = executable_network.infer({'inp_img': transformed_img})\n",
    "arr_key = list(openvino_output.keys())[0] ### get key for putput image\n",
    "openvino_output_img = torch.from_numpy(np.asarray(openvino_output[arr_key]))\n",
    "openvino_output_image = tensor2im(openvino_output_img) ### final OpenVino result image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (30, 30))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.gca().set_title('Pytorch Output', fontsize = 'xx-large')\n",
    "plt.imshow(output_image)\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.gca().set_title('OpenVino Output', fontsize = 'xx-large')\n",
    "plt.imshow(openvino_output_image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
